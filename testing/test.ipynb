{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, TextClip, ImageClip, CompositeVideoClip, AudioFileClip\n",
    "import pyttsx3 as tts\n",
    "import whisper\n",
    "import requests\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import generate, save\n",
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "client = ElevenLabs(api_key=os.getenv('ELEVENLABS_API_KEY'))\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_script(prompt, stories):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    default_prompt = \"\"\"from now on return the output as regular text and the story should look\n",
    "                    like it being narrated by someone, For an Example : There was once a boy fishing in the ocean\n",
    "                    and he caught a fish, the fish was very big he was so happy, \n",
    "                    You have to write maximum 1000 characters I want a plain text no symbols just the script, \n",
    "                    also the script content must be in 50 seconds duration, dont include any symbols or special characters,\"\"\"\n",
    "    for i in range(stories):\n",
    "        story = model.generate_content(f\"{default_prompt},{prompt}\")\n",
    "        with open(f'stories/story_{i}.txt', 'w') as f:\n",
    "            f.write(story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(text):\n",
    "    BASE = 'https://api.unsplash.com/photos/'\n",
    "    PAYLOAD = {\n",
    "        'query': text,\n",
    "        'client_id': os.getenv('UNSPLASH_ACCESS_KEY')\n",
    "    }\n",
    "    response = requests.get(BASE, params=PAYLOAD)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Scary Mystery Puzzle\"\n",
    "stories = 5\n",
    "generate_script(prompt, stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def speak(text):\n",
    "#     audio = generate(text,\n",
    "#                      voice='Rachel',\n",
    "#                      model = \"eleven_multilingual_v2\")\n",
    "#     save(audio, 'samp.mp3')\n",
    "    \n",
    "def speak(text, file_name):\n",
    "    engine = tts.init()\n",
    "    engine.setProperty('rate', 150)\n",
    "    engine.save_to_file(text, f'audio/{file_name}.mp3')\n",
    "    engine.runAndWait()\n",
    "\n",
    "for i in range(stories):\n",
    "    with open(f'stories/story_{i}.txt', 'r') as f:\n",
    "        text = f.readlines()\n",
    "        speak(text, f'story_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    data = model.transcribe(audio_file, word_timestamps=True)\n",
    "\n",
    "    start = [data['segments'][i]['words'][j]['start'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    end = [data['segments'][i]['words'][j]['end'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    text = [data['segments'][i]['words'][j]['word'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "\n",
    "    return [start, end, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mspeech\u001b[49m[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(((speech[\u001b[38;5;241m0\u001b[39m][l], speech[\u001b[38;5;241m1\u001b[39m][l]), speech[\u001b[38;5;241m2\u001b[39m][l]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech' is not defined"
     ]
    }
   ],
   "source": [
    "for l in range(len(speech[0])):\n",
    "    print(((speech[0][l], speech[1][l]), speech[2][l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "\n",
    "def generate_video():\n",
    "    for i in range(stories):\n",
    "        with open(f'stories/story_{i}.txt', 'r') as file:\n",
    "            print(f'Generating video for story {i}')\n",
    "            text = file.read()\n",
    "        \n",
    "        lst = text.split(' ')\n",
    "\n",
    "        #clips = []\n",
    "\n",
    "        generator = lambda txt: TextClip(txt, \n",
    "                                    fontsize=70, \n",
    "                                    color='white', \n",
    "                                    bg_color='none', \n",
    "                                    font='Arial-Bold',\n",
    "                                    method = 'caption',\n",
    "                                    size = (1920, 1080))\n",
    "\n",
    "        clips = [CompositeVideoClip([ImageClip('Z_1.jpg').set_duration(0.5).set_position('center')], size=(720, 1280)) for _ in lst]\n",
    "        speech = speech_to_text(f'audio/story_{i}.mp3')\n",
    "        print(f'Accessing speech for story {i}')\n",
    "        subs = [((speech[0][k], speech[1][k]), speech[2][k]) for k in range(len(speech[0]))]\n",
    "\n",
    "        subtitles = SubtitlesClip(subs, generator)\n",
    "\n",
    "        final = concatenate_videoclips(clips, method=\"compose\")\n",
    "        audio = AudioFileClip(f\"audio/story_{i}.mp3\")\n",
    "        final = final.set_audio(audio)\n",
    "        final_complete = CompositeVideoClip([final, subtitles.set_position(('center', 'bottom'))])\n",
    "        \n",
    "        final_complete.write_videofile(f\"output_{i}.mp4\", fps=24, bitrate = '1000', audio_codec = 'aac', preset = 'fast', codec = 'h264_nvenc', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for story 0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36mgenerate_video\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#clips = []\u001b[39;00m\n\u001b[0;32m     13\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m txt: TextClip(txt, \n\u001b[0;32m     14\u001b[0m                             fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, \n\u001b[0;32m     15\u001b[0m                             color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m                             method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m                             size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m1080\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m clips \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mCompositeVideoClip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageClip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZ_1.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcenter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m speech \u001b[38;5;241m=\u001b[39m speech_to_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio/story_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccessing speech for story \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#clips = []\u001b[39;00m\n\u001b[0;32m     13\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m txt: TextClip(txt, \n\u001b[0;32m     14\u001b[0m                             fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, \n\u001b[0;32m     15\u001b[0m                             color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m                             method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m                             size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m1080\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m clips \u001b[38;5;241m=\u001b[39m [CompositeVideoClip([\u001b[43mImageClip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZ_1.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_duration(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mset_position(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m)], size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m720\u001b[39m, \u001b[38;5;241m1280\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[0;32m     22\u001b[0m speech \u001b[38;5;241m=\u001b[39m speech_to_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio/story_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccessing speech for story \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:889\u001b[0m, in \u001b[0;36mImageClip.__init__\u001b[1;34m(self, img, ismask, transparent, fromalpha, duration)\u001b[0m\n\u001b[0;32m    886\u001b[0m VideoClip\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ismask\u001b[38;5;241m=\u001b[39mismask, duration\u001b[38;5;241m=\u001b[39mduration)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, string_types):\n\u001b[1;32m--> 889\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# img is (now) a RGB(a) numpy array\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\imageio\\__init__.py:97\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     95\u001b[0m )\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimread_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\imageio\\v2.py:360\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 360\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\imageio\\plugins\\pillow.py:252\u001b[0m, in \u001b[0;36mPillowPlugin.read\u001b[1;34m(self, index, mode, rotate, apply_gamma, writeable_output, pilmode, exifrotate, as_gray)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# will raise IO error if index >= number of frames in image\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image\u001b[38;5;241m.\u001b[39mseek(index)\n\u001b[1;32m--> 252\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriteable_output\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(\n\u001b[0;32m    257\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    258\u001b[0m         rotate\u001b[38;5;241m=\u001b[39mrotate,\n\u001b[0;32m    259\u001b[0m         apply_gamma\u001b[38;5;241m=\u001b[39mapply_gamma,\n\u001b[0;32m    260\u001b[0m         writeable_output\u001b[38;5;241m=\u001b[39mwriteable_output,\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\imageio\\plugins\\pillow.py:333\u001b[0m, in \u001b[0;36mPillowPlugin._apply_transforms\u001b[1;34m(self, image, mode, rotate, apply_gamma, writeable_output)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;66;03m# pillow >= 10.1.0\u001b[39;00m\n\u001b[0;32m    331\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(desired_mode)\n\u001b[1;32m--> 333\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image)\n\u001b[0;32m    335\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image\u001b[38;5;241m.\u001b[39mtell(), exclude_applied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rotate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrientation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meta:\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\PIL\\Image.py:681\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\PIL\\Image.py:761\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    758\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in tobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m--> 761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tortoise:\n",
      "\n",
      "NAME\n",
      "    tortoise\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    api_fast\n",
      "    do_tts\n",
      "    eval\n",
      "    get_conditioning_latents\n",
      "    is_this_from_tortoise\n",
      "    models (package)\n",
      "    read\n",
      "    read_fast\n",
      "    tts_stream\n",
      "    utils (package)\n",
      "\n",
      "FILE\n",
      "    d:\\projects\\video-generator\\myenv\\lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tortoise as t\n",
    "from tortoise import api_fast\n",
    "\n",
    "help(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mapi_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextToSpeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pcm_audio \u001b[38;5;241m=\u001b[39m tts\u001b[38;5;241m.\u001b[39mtts_with_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour text here\u001b[39m\u001b[38;5;124m\"\u001b[39m, preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfast\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\api_fast.py:216\u001b[0m, in \u001b[0;36mTextToSpeech.__init__\u001b[1;34m(self, autoregressive_batch_size, models_dir, enable_redaction, kv_cache, use_deepspeed, half, device, tokenizer_vocab_file, tokenizer_basic)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive \u001b[38;5;241m=\u001b[39m UnifiedVoice(max_mel_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m604\u001b[39m, max_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m402\u001b[39m, max_conditioning_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    212\u001b[0m                                   model_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    213\u001b[0m                                   heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, number_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, start_text_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    214\u001b[0m                                   train_solo_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoregressive.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, models_dir)), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoregressive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init_gpt2_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deepspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhifi_decoder \u001b[38;5;241m=\u001b[39m HifiganGenerator(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, resblock_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m resblock_dilation_sizes \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]], resblock_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m11\u001b[39m],\n\u001b[0;32m    220\u001b[0m upsample_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m], upsample_initial_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, upsample_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m    221\u001b[0m cond_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    222\u001b[0m hifi_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhifidecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\models\\autoregressive.py:380\u001b[0m, in \u001b[0;36mUnifiedVoice.post_init_gpt2_config\u001b[1;34m(self, use_deepspeed, kv_cache, half)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m GPT2InferenceModel(\n\u001b[0;32m    371\u001b[0m     gpt_config,\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m     kv_cache\u001b[38;5;241m=\u001b[39mkv_cache,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_deepspeed \u001b[38;5;129;01mand\u001b[39;00m half \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine \u001b[38;5;241m=\u001b[39m deepspeed\u001b[38;5;241m.\u001b[39minit_inference(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model,  \n\u001b[0;32m    382\u001b[0m                                             mp_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    383\u001b[0m                                             replace_with_kernel_inject\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    384\u001b[0m                                             dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "tts = api_fast.TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)\n",
    "pcm_audio = tts.tts_with_preset(\"your text here\", preset='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
