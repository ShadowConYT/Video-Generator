{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "status_code: 400, body: {'detail': {'status': 'max_character_limit_exceeded', 'message': \"This request's text has 1007.0 characters and exceeds the character limit of 2500 characters for non signed in accounts.\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamp.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     24\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspeak\u001b[39m(text):\n\u001b[1;32m---> 13\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvoice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRachel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meleven_multilingual_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     save(audio, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamp.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\elevenlabs\\generate.py:57\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(text, api_key, voice, model, stream, latency, stream_chunk_size, output_format)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\elevenlabs\\tts.py:54\u001b[0m, in \u001b[0;36mTTS.generate\u001b[1;34m(text, voice, model, api_key, output_format, latency)\u001b[0m\n\u001b[0;32m     46\u001b[0m client \u001b[38;5;241m=\u001b[39m ElevenLabs(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m     47\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtext_to_speech\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m     48\u001b[0m     voice\u001b[38;5;241m.\u001b[39mvoice_id,\n\u001b[0;32m     49\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     optimize_streaming_latency\u001b[38;5;241m=\u001b[39mlatency,\n\u001b[0;32m     53\u001b[0m     output_format\u001b[38;5;241m=\u001b[39moutput_format)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\elevenlabs\\resources\\text_to_speech\\client.py:87\u001b[0m, in \u001b[0;36mTextToSpeechClient.convert\u001b[1;34m(self, voice_id, optimize_streaming_latency, output_format, text, model_id, voice_settings)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[1;31mApiError\u001b[0m: status_code: 400, body: {'detail': {'status': 'max_character_limit_exceeded', 'message': \"This request's text has 1007.0 characters and exceeds the character limit of 2500 characters for non signed in accounts.\"}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, TextClip, ImageClip, CompositeVideoClip, AudioFileClip\n",
    "import pyttsx3 as tts\n",
    "import whisper\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import generate, save\n",
    "load_dotenv()\n",
    "\n",
    "client = ElevenLabs(api_key=os.getenv('ELEVENLABS_API_KEY'))\n",
    "\n",
    "def speak(text):\n",
    "    audio = generate(text,\n",
    "                     voice='Rachel',\n",
    "                     model = \"eleven_multilingual_v2\")\n",
    "    save(audio, 'samp.mp3')\n",
    "    \n",
    "\n",
    "def get_files_in_directory(directory):\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "\n",
    "with open('samp.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    speak(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    data = model.transcribe(audio_file, word_timestamps=True)\n",
    "\n",
    "    start = [data['segments'][i]['words'][j]['start'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    end = [data['segments'][i]['words'][j]['end'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    text = [data['segments'][i]['words'][j]['word'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "\n",
    "    return [start, end, text]\n",
    "\n",
    "speech = speech_to_text(\"samp.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.0, 0.22), ' As')\n",
      "((0.22, 0.3), ' the')\n",
      "((0.3, 0.48), ' moon')\n",
      "((0.48, 0.68), ' cast')\n",
      "((0.68, 0.9), ' an')\n",
      "((0.9, 1.08), ' eerie')\n",
      "((1.08, 1.26), ' glow')\n",
      "((1.26, 1.52), ' over')\n",
      "((1.52, 1.66), ' the')\n",
      "((1.66, 1.96), ' desolate')\n",
      "((1.96, 2.28), ' streets,')\n",
      "((2.78, 2.86), ' a')\n",
      "((2.86, 3.0), ' lone')\n",
      "((3.0, 3.26), ' figure')\n",
      "((3.26, 3.56), ' emerges')\n",
      "((3.56, 3.84), ' from')\n",
      "((3.84, 4.0), ' the')\n",
      "((4.0, 4.26), ' shadows,')\n",
      "((4.78, 4.88), ' their')\n",
      "((4.88, 5.12), ' footsteps')\n",
      "((5.12, 5.68), ' echoing')\n",
      "((5.68, 5.8), ' like')\n",
      "((5.8, 5.94), ' a')\n",
      "((5.94, 6.16), ' sinister')\n",
      "((6.16, 6.56), ' heartbeat.')\n",
      "((7.28, 7.52), ' In')\n",
      "((7.52, 7.62), ' the')\n",
      "((7.62, 7.76), ' dim')\n",
      "((7.76, 7.94), ' light,')\n",
      "((8.36, 8.48), ' we')\n",
      "((8.48, 8.7), ' catch')\n",
      "((8.7, 9.06), ' glimpses')\n",
      "((9.06, 9.22), ' of')\n",
      "((9.22, 9.34), ' their')\n",
      "((9.34, 9.56), ' haunted')\n",
      "((9.56, 10.02), ' expression,')\n",
      "((10.48, 10.58), ' a')\n",
      "((10.58, 10.86), ' reflection')\n",
      "((10.86, 11.1), ' of')\n",
      "((11.1, 11.2), ' the')\n",
      "((11.2, 11.44), ' torment')\n",
      "((11.44, 11.76), ' within.')\n",
      "((12.54, 12.72), ' They')\n",
      "((12.72, 12.96), ' carry')\n",
      "((12.96, 13.08), ' a')\n",
      "((13.08, 13.26), ' burden')\n",
      "((13.26, 13.42), ' of')\n",
      "((13.42, 13.7), ' secrets,')\n",
      "((14.2, 14.4), ' each')\n",
      "((14.4, 14.6), ' step')\n",
      "((14.6, 14.84), ' drawing')\n",
      "((14.84, 15.0), ' them')\n",
      "((15.0, 15.28), ' closer')\n",
      "((15.28, 15.5), ' to')\n",
      "((15.5, 15.6), ' a')\n",
      "((15.6, 15.8), ' chilling')\n",
      "((15.8, 16.22), ' revelation.')\n",
      "((16.98, 17.22), ' In')\n",
      "((17.22, 17.3), ' a')\n",
      "((17.3, 17.48), ' city')\n",
      "((17.48, 17.84), ' plagued')\n",
      "((17.84, 17.94), ' by')\n",
      "((17.94, 18.22), ' darkness,')\n",
      "((18.68, 18.84), ' where')\n",
      "((18.84, 19.1), ' every')\n",
      "((19.1, 19.32), ' corner')\n",
      "((19.32, 19.6), ' holds')\n",
      "((19.6, 19.76), ' a')\n",
      "((19.76, 19.88), ' new')\n",
      "((19.88, 20.12), ' terror,')\n",
      "((20.56, 20.7), ' this')\n",
      "((20.7, 20.94), ' figure')\n",
      "((20.94, 21.42), ' navigates')\n",
      "((21.42, 21.54), ' the')\n",
      "((21.54, 21.92), ' labyrinth')\n",
      "((21.92, 21.98), ' of')\n",
      "((21.98, 22.16), ' fear')\n",
      "((22.16, 22.34), ' with')\n",
      "((22.34, 22.4), ' a')\n",
      "((22.4, 22.8), ' determination')\n",
      "((22.8, 23.16), ' born')\n",
      "((23.16, 23.36), ' from')\n",
      "((23.36, 23.82), ' desperation.')\n",
      "((24.58, 24.88), ' But')\n",
      "((24.88, 25.04), ' as')\n",
      "((25.04, 25.14), ' they')\n",
      "((25.14, 25.28), ' delve')\n",
      "((25.28, 25.58), ' deeper')\n",
      "((25.58, 25.82), ' into')\n",
      "((25.82, 25.94), ' the')\n",
      "((25.94, 26.2), ' mystery')\n",
      "((26.2, 26.66), ' shrouding')\n",
      "((26.66, 26.8), ' their')\n",
      "((26.8, 27.04), ' past,')\n",
      "((27.04, 27.62), ' they')\n",
      "((27.62, 27.84), ' soon')\n",
      "((27.84, 28.16), ' realize')\n",
      "((28.16, 28.42), ' that')\n",
      "((28.42, 28.64), ' some')\n",
      "((28.64, 28.9), ' secrets')\n",
      "((28.9, 29.12), ' are')\n",
      "((29.12, 29.32), ' best')\n",
      "((29.32, 29.58), ' left')\n",
      "((29.58, 29.88), ' buried.')\n",
      "((30.36, 30.76), ' With')\n",
      "((30.76, 30.96), ' each')\n",
      "((30.96, 31.24), ' passing')\n",
      "((31.24, 31.54), ' moment,')\n",
      "((31.98, 32.06), ' the')\n",
      "((32.06, 32.2), ' line')\n",
      "((32.2, 32.48), ' between')\n",
      "((32.48, 32.92), ' reality')\n",
      "((32.92, 33.12), ' and')\n",
      "((33.12, 33.38), ' nightmare')\n",
      "((33.38, 33.8), ' blurs,')\n",
      "((34.16, 34.3), ' and')\n",
      "((34.3, 34.38), ' the')\n",
      "((34.38, 34.56), ' truth')\n",
      "((34.56, 34.88), ' becomes')\n",
      "((34.88, 35.06), ' a')\n",
      "((35.06, 35.32), ' sinister')\n",
      "((35.32, 35.8), ' specter')\n",
      "((35.8, 35.98), ' haunting')\n",
      "((35.98, 36.22), ' their')\n",
      "((36.22, 36.44), ' every')\n",
      "((36.44, 36.72), ' move.')\n",
      "((37.38, 37.58), ' In')\n",
      "((37.58, 37.72), ' this')\n",
      "((37.72, 38.04), ' twisted')\n",
      "((38.04, 38.28), ' game')\n",
      "((38.28, 38.44), ' of')\n",
      "((38.44, 38.82), ' survival,')\n",
      "((39.24, 39.38), ' where')\n",
      "((39.38, 39.68), ' shadows')\n",
      "((39.68, 40.02), ' whisper')\n",
      "((40.02, 40.24), ' and')\n",
      "((40.24, 40.34), ' the')\n",
      "((40.34, 40.46), ' night')\n",
      "((40.46, 40.68), ' holds')\n",
      "((40.68, 40.84), ' its')\n",
      "((40.84, 41.12), ' breath,')\n",
      "((41.56, 41.66), ' the')\n",
      "((41.66, 41.86), ' only')\n",
      "((41.86, 42.28), ' certainty')\n",
      "((42.28, 42.48), ' is')\n",
      "((42.48, 42.58), ' the')\n",
      "((42.58, 42.86), ' relentless')\n",
      "((42.86, 43.24), ' pursuit')\n",
      "((43.24, 43.4), ' of')\n",
      "((43.4, 43.52), ' the')\n",
      "((43.52, 43.76), ' unknown.')\n",
      "((44.48, 44.74), ' And')\n",
      "((44.74, 44.9), ' as')\n",
      "((44.9, 45.0), ' the')\n",
      "((45.0, 45.26), ' story')\n",
      "((45.26, 45.84), ' unfolds,')\n",
      "((46.14, 46.28), ' the')\n",
      "((46.28, 46.5), ' chilling')\n",
      "((46.5, 46.78), ' truth')\n",
      "((46.78, 47.08), ' emerges,')\n",
      "((47.58, 47.68), ' in')\n",
      "((47.68, 47.78), ' the')\n",
      "((47.78, 47.98), ' city')\n",
      "((47.98, 48.12), ' of')\n",
      "((48.12, 48.4), ' shadows,')\n",
      "((48.88, 49.06), ' no')\n",
      "((49.06, 49.22), ' one')\n",
      "((49.22, 49.36), ' is')\n",
      "((49.36, 49.6), ' truly')\n",
      "((49.6, 50.0), ' safe.')\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(speech[0])):\n",
    "    print(((speech[0][l], speech[1][l]), speech[2][l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "\n",
    "def generate_video(): \n",
    "    with open('samp.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    lst = text.split(' ')\n",
    "\n",
    "    clips = []\n",
    "\n",
    "    generator = lambda txt: TextClip(txt, \n",
    "                                fontsize=70, \n",
    "                                color='white', \n",
    "                                bg_color='none', \n",
    "                                font='Arial-Bold',\n",
    "                                method = 'caption',\n",
    "                                size = (1920, 1080))\n",
    "\n",
    "    clips = [CompositeVideoClip([ImageClip('Z_1.jpg').set_duration(0.5).set_position('center')], size=(1920, 1080)) for _ in lst]\n",
    "\n",
    "    subs = [((speech[0][k], speech[1][k]), speech[2][k]) for k in range(len(speech[0]))]\n",
    "        \n",
    "\n",
    "    subtitles = SubtitlesClip(subs, generator)\n",
    "\n",
    "    final = concatenate_videoclips(clips)\n",
    "    audio = AudioFileClip(\"samp.mp3\")\n",
    "    final = final.set_audio(audio)\n",
    "    final_complete = CompositeVideoClip([final, subtitles.set_position(('center', 'bottom'))])\n",
    "    final_complete.write_videofile(\"output0.mp4\", fps=24, codec = 'libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output0.mp4.\n",
      "MoviePy - Writing audio in output0TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output0.mp4\n"
     ]
    }
   ],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tortoise as t\n",
    "from tortoise import api\n",
    "\n",
    "help(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241m.\u001b[39mTextToSpeech()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "tts = api.TextToSpeech()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
