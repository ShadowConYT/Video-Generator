{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, TextClip, ImageClip, CompositeVideoClip, AudioFileClip\n",
    "import pyttsx3 as tts\n",
    "import whisper\n",
    "import requests\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import generate, save\n",
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "client = ElevenLabs(api_key=os.getenv('ELEVENLABS_API_KEY'))\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_script(prompt, stories):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    default_prompt = \"\"\"from now on return the output as regular text and the story should look\n",
    "                    like it being narrated by someone, For an Example : There was once a boy fishing in the ocean\n",
    "                    and he caught a fish, the fish was very big he was so happy, \n",
    "                    You have to write maximum 1000 characters I want a plain text no symbols just the script, \n",
    "                    also the script content must be in 50 seconds duration, dont include any symbols or special characters,\"\"\"\n",
    "    for i in range(stories):\n",
    "        story = model.generate_content(f\"{default_prompt},{prompt}\")\n",
    "        with open(f'stories/story_{i}.txt', 'w') as f:\n",
    "            f.write(story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(text):\n",
    "    BASE = 'https://api.unsplash.com/photos/'\n",
    "    PAYLOAD = {\n",
    "        'query': text,\n",
    "        'client_id': os.getenv('UNSPLASH_ACCESS_KEY')\n",
    "    }\n",
    "    response = requests.get(BASE, params=PAYLOAD)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Scary Mystery Puzzle\"\n",
    "stories = 5\n",
    "generate_script(prompt, stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def speak(text):\n",
    "#     audio = generate(text,\n",
    "#                      voice='Rachel',\n",
    "#                      model = \"eleven_multilingual_v2\")\n",
    "#     save(audio, 'samp.mp3')\n",
    "    \n",
    "def speak(text, file_name):\n",
    "    engine = tts.init()\n",
    "    engine.setProperty('rate', 150)\n",
    "    engine.save_to_file(text, f'audio/{file_name}.mp3')\n",
    "    engine.runAndWait()\n",
    "\n",
    "for i in range(stories):\n",
    "    with open(f'stories/story_{i}.txt', 'r') as f:\n",
    "        text = f.readlines()\n",
    "        speak(text, f'story_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    data = model.transcribe(audio_file, word_timestamps=True)\n",
    "\n",
    "    start = [data['segments'][i]['words'][j]['start'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    end = [data['segments'][i]['words'][j]['end'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    text = [data['segments'][i]['words'][j]['word'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "\n",
    "    return [start, end, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mspeech\u001b[49m[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(((speech[\u001b[38;5;241m0\u001b[39m][l], speech[\u001b[38;5;241m1\u001b[39m][l]), speech[\u001b[38;5;241m2\u001b[39m][l]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech' is not defined"
     ]
    }
   ],
   "source": [
    "for l in range(len(speech[0])):\n",
    "    print(((speech[0][l], speech[1][l]), speech[2][l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "\n",
    "def generate_video():\n",
    "    for i in range(stories):\n",
    "        with open(f'stories/story_{i}.txt', 'r') as file:\n",
    "            print(f'Generating video for story {i}')\n",
    "            text = file.read()\n",
    "        \n",
    "        lst = text.split(' ')\n",
    "\n",
    "        #clips = []\n",
    "\n",
    "        generator = lambda txt: TextClip(txt, \n",
    "                                    fontsize=70, \n",
    "                                    color='white', \n",
    "                                    bg_color='none', \n",
    "                                    font='Arial-Bold',\n",
    "                                    method = 'caption',\n",
    "                                    size = (1920, 1080))\n",
    "\n",
    "        clips = [CompositeVideoClip([ImageClip('Z_1.jpg').set_duration(0.5).set_position('center')], size=(720, 1280)) for _ in lst]\n",
    "        speech = speech_to_text(f'audio/story_{i}.mp3')\n",
    "        print(f'Accessing speech for story {i}')\n",
    "        subs = [((speech[0][k], speech[1][k]), speech[2][k]) for k in range(len(speech[0]))]\n",
    "\n",
    "        subtitles = SubtitlesClip(subs, generator)\n",
    "\n",
    "        final = concatenate_videoclips(clips, method=\"compose\")\n",
    "        audio = AudioFileClip(f\"audio/story_{i}.mp3\")\n",
    "        final = final.set_audio(audio)\n",
    "        final_complete = CompositeVideoClip([final, subtitles.set_position(('center', 'bottom'))])\n",
    "        \n",
    "        final_complete.write_videofile(f\"output_{i}.mp4\", fps=24, bitrate = '1000', audio_codec = 'aac', preset = 'fast', codec = 'h264_nvenc', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for story 0\n"
     ]
    }
   ],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tortoise:\n",
      "\n",
      "NAME\n",
      "    tortoise\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    api_fast\n",
      "    do_tts\n",
      "    eval\n",
      "    get_conditioning_latents\n",
      "    is_this_from_tortoise\n",
      "    models (package)\n",
      "    read\n",
      "    read_fast\n",
      "    tts_stream\n",
      "    utils (package)\n",
      "\n",
      "FILE\n",
      "    d:\\projects\\video-generator\\myenv\\lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tortoise as t\n",
    "from tortoise import api_fast\n",
    "\n",
    "help(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mapi_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextToSpeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pcm_audio \u001b[38;5;241m=\u001b[39m tts\u001b[38;5;241m.\u001b[39mtts_with_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour text here\u001b[39m\u001b[38;5;124m\"\u001b[39m, preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfast\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\api_fast.py:216\u001b[0m, in \u001b[0;36mTextToSpeech.__init__\u001b[1;34m(self, autoregressive_batch_size, models_dir, enable_redaction, kv_cache, use_deepspeed, half, device, tokenizer_vocab_file, tokenizer_basic)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive \u001b[38;5;241m=\u001b[39m UnifiedVoice(max_mel_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m604\u001b[39m, max_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m402\u001b[39m, max_conditioning_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    212\u001b[0m                                   model_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    213\u001b[0m                                   heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, number_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, start_text_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    214\u001b[0m                                   train_solo_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoregressive.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, models_dir)), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoregressive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init_gpt2_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deepspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhifi_decoder \u001b[38;5;241m=\u001b[39m HifiganGenerator(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, resblock_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m resblock_dilation_sizes \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]], resblock_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m11\u001b[39m],\n\u001b[0;32m    220\u001b[0m upsample_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m], upsample_initial_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, upsample_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m    221\u001b[0m cond_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    222\u001b[0m hifi_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhifidecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\models\\autoregressive.py:380\u001b[0m, in \u001b[0;36mUnifiedVoice.post_init_gpt2_config\u001b[1;34m(self, use_deepspeed, kv_cache, half)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m GPT2InferenceModel(\n\u001b[0;32m    371\u001b[0m     gpt_config,\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m     kv_cache\u001b[38;5;241m=\u001b[39mkv_cache,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_deepspeed \u001b[38;5;129;01mand\u001b[39;00m half \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine \u001b[38;5;241m=\u001b[39m deepspeed\u001b[38;5;241m.\u001b[39minit_inference(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model,  \n\u001b[0;32m    382\u001b[0m                                             mp_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    383\u001b[0m                                             replace_with_kernel_inject\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    384\u001b[0m                                             dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "tts = api_fast.TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)\n",
    "pcm_audio = tts.tts_with_preset(\"your text here\", preset='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
