{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, TextClip, ImageClip, CompositeVideoClip, AudioFileClip\n",
    "import pyttsx3 as tts\n",
    "import whisper\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import generate, save\n",
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "client = ElevenLabs(api_key=os.getenv('ELEVENLABS_API_KEY'))\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_script(prompt, stories):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    default_prompt = \"\"\"from now on return the output as regular text and the story should look\n",
    "                    like it being narrated by someone, For an Example : There was once a boy fishing in the ocean\n",
    "                    and he caught a fish, the fish was very big he was so happy, \n",
    "                    You have to write maximum 1000 characters I want a plain text no symbols just the script, \n",
    "                    also the script content must be in 50 seconds duration\"\"\"\n",
    "    for i in range(stories):\n",
    "        story = model.generate_content(f\"{default_prompt},{prompt}\")\n",
    "        with open(f'story_{i}.txt', 'w') as f:\n",
    "            f.write(story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Scary Mystery Puzzle\"\n",
    "stories = 5\n",
    "generate_script(prompt, stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def speak(text):\n",
    "#     audio = generate(text,\n",
    "#                      voice='Rachel',\n",
    "#                      model = \"eleven_multilingual_v2\")\n",
    "#     save(audio, 'samp.mp3')\n",
    "    \n",
    "def speak(text):\n",
    "    engine = tts.init()\n",
    "    engine.setProperty('rate', 150)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    engine.save_to_file(text, 'samp.mp3')\n",
    "\n",
    "def get_files_in_directory(directory):\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "\n",
    "with open('story_0.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    speak(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    data = model.transcribe(audio_file, word_timestamps=True)\n",
    "\n",
    "    start = [data['segments'][i]['words'][j]['start'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    end = [data['segments'][i]['words'][j]['end'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "    text = [data['segments'][i]['words'][j]['word'] for i in range(len(data['segments'])) for j in range(len(data['segments'][i]['words']))]\n",
    "\n",
    "    return [start, end, text]\n",
    "\n",
    "speech = speech_to_text(\"samp.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.0, 0.22), ' As')\n",
      "((0.22, 0.3), ' the')\n",
      "((0.3, 0.48), ' moon')\n",
      "((0.48, 0.68), ' cast')\n",
      "((0.68, 0.9), ' an')\n",
      "((0.9, 1.08), ' eerie')\n",
      "((1.08, 1.26), ' glow')\n",
      "((1.26, 1.52), ' over')\n",
      "((1.52, 1.66), ' the')\n",
      "((1.66, 1.96), ' desolate')\n",
      "((1.96, 2.28), ' streets,')\n",
      "((2.78, 2.86), ' a')\n",
      "((2.86, 3.0), ' lone')\n",
      "((3.0, 3.26), ' figure')\n",
      "((3.26, 3.56), ' emerges')\n",
      "((3.56, 3.84), ' from')\n",
      "((3.84, 4.0), ' the')\n",
      "((4.0, 4.26), ' shadows,')\n",
      "((4.78, 4.88), ' their')\n",
      "((4.88, 5.12), ' footsteps')\n",
      "((5.12, 5.68), ' echoing')\n",
      "((5.68, 5.8), ' like')\n",
      "((5.8, 5.94), ' a')\n",
      "((5.94, 6.16), ' sinister')\n",
      "((6.16, 6.56), ' heartbeat.')\n",
      "((7.28, 7.52), ' In')\n",
      "((7.52, 7.62), ' the')\n",
      "((7.62, 7.76), ' dim')\n",
      "((7.76, 7.94), ' light,')\n",
      "((8.36, 8.48), ' we')\n",
      "((8.48, 8.7), ' catch')\n",
      "((8.7, 9.06), ' glimpses')\n",
      "((9.06, 9.22), ' of')\n",
      "((9.22, 9.34), ' their')\n",
      "((9.34, 9.56), ' haunted')\n",
      "((9.56, 10.02), ' expression,')\n",
      "((10.48, 10.58), ' a')\n",
      "((10.58, 10.86), ' reflection')\n",
      "((10.86, 11.1), ' of')\n",
      "((11.1, 11.2), ' the')\n",
      "((11.2, 11.44), ' torment')\n",
      "((11.44, 11.76), ' within.')\n",
      "((12.54, 12.72), ' They')\n",
      "((12.72, 12.96), ' carry')\n",
      "((12.96, 13.08), ' a')\n",
      "((13.08, 13.26), ' burden')\n",
      "((13.26, 13.42), ' of')\n",
      "((13.42, 13.7), ' secrets,')\n",
      "((14.2, 14.4), ' each')\n",
      "((14.4, 14.6), ' step')\n",
      "((14.6, 14.84), ' drawing')\n",
      "((14.84, 15.0), ' them')\n",
      "((15.0, 15.28), ' closer')\n",
      "((15.28, 15.5), ' to')\n",
      "((15.5, 15.6), ' a')\n",
      "((15.6, 15.8), ' chilling')\n",
      "((15.8, 16.22), ' revelation.')\n",
      "((16.98, 17.22), ' In')\n",
      "((17.22, 17.3), ' a')\n",
      "((17.3, 17.48), ' city')\n",
      "((17.48, 17.84), ' plagued')\n",
      "((17.84, 17.94), ' by')\n",
      "((17.94, 18.22), ' darkness,')\n",
      "((18.68, 18.84), ' where')\n",
      "((18.84, 19.1), ' every')\n",
      "((19.1, 19.32), ' corner')\n",
      "((19.32, 19.6), ' holds')\n",
      "((19.6, 19.76), ' a')\n",
      "((19.76, 19.88), ' new')\n",
      "((19.88, 20.12), ' terror,')\n",
      "((20.56, 20.7), ' this')\n",
      "((20.7, 20.94), ' figure')\n",
      "((20.94, 21.42), ' navigates')\n",
      "((21.42, 21.54), ' the')\n",
      "((21.54, 21.92), ' labyrinth')\n",
      "((21.92, 21.98), ' of')\n",
      "((21.98, 22.16), ' fear')\n",
      "((22.16, 22.34), ' with')\n",
      "((22.34, 22.4), ' a')\n",
      "((22.4, 22.8), ' determination')\n",
      "((22.8, 23.16), ' born')\n",
      "((23.16, 23.36), ' from')\n",
      "((23.36, 23.82), ' desperation.')\n",
      "((24.58, 24.88), ' But')\n",
      "((24.88, 25.04), ' as')\n",
      "((25.04, 25.14), ' they')\n",
      "((25.14, 25.28), ' delve')\n",
      "((25.28, 25.58), ' deeper')\n",
      "((25.58, 25.82), ' into')\n",
      "((25.82, 25.94), ' the')\n",
      "((25.94, 26.2), ' mystery')\n",
      "((26.2, 26.66), ' shrouding')\n",
      "((26.66, 26.8), ' their')\n",
      "((26.8, 27.04), ' past,')\n",
      "((27.04, 27.62), ' they')\n",
      "((27.62, 27.84), ' soon')\n",
      "((27.84, 28.16), ' realize')\n",
      "((28.16, 28.42), ' that')\n",
      "((28.42, 28.64), ' some')\n",
      "((28.64, 28.9), ' secrets')\n",
      "((28.9, 29.12), ' are')\n",
      "((29.12, 29.32), ' best')\n",
      "((29.32, 29.58), ' left')\n",
      "((29.58, 29.88), ' buried.')\n",
      "((30.36, 30.76), ' With')\n",
      "((30.76, 30.96), ' each')\n",
      "((30.96, 31.24), ' passing')\n",
      "((31.24, 31.54), ' moment,')\n",
      "((31.98, 32.06), ' the')\n",
      "((32.06, 32.2), ' line')\n",
      "((32.2, 32.48), ' between')\n",
      "((32.48, 32.92), ' reality')\n",
      "((32.92, 33.12), ' and')\n",
      "((33.12, 33.38), ' nightmare')\n",
      "((33.38, 33.8), ' blurs,')\n",
      "((34.16, 34.3), ' and')\n",
      "((34.3, 34.38), ' the')\n",
      "((34.38, 34.56), ' truth')\n",
      "((34.56, 34.88), ' becomes')\n",
      "((34.88, 35.06), ' a')\n",
      "((35.06, 35.32), ' sinister')\n",
      "((35.32, 35.8), ' specter')\n",
      "((35.8, 35.98), ' haunting')\n",
      "((35.98, 36.22), ' their')\n",
      "((36.22, 36.44), ' every')\n",
      "((36.44, 36.72), ' move.')\n",
      "((37.38, 37.58), ' In')\n",
      "((37.58, 37.72), ' this')\n",
      "((37.72, 38.04), ' twisted')\n",
      "((38.04, 38.28), ' game')\n",
      "((38.28, 38.44), ' of')\n",
      "((38.44, 38.82), ' survival,')\n",
      "((39.24, 39.38), ' where')\n",
      "((39.38, 39.68), ' shadows')\n",
      "((39.68, 40.02), ' whisper')\n",
      "((40.02, 40.24), ' and')\n",
      "((40.24, 40.34), ' the')\n",
      "((40.34, 40.46), ' night')\n",
      "((40.46, 40.68), ' holds')\n",
      "((40.68, 40.84), ' its')\n",
      "((40.84, 41.12), ' breath,')\n",
      "((41.56, 41.66), ' the')\n",
      "((41.66, 41.86), ' only')\n",
      "((41.86, 42.28), ' certainty')\n",
      "((42.28, 42.48), ' is')\n",
      "((42.48, 42.58), ' the')\n",
      "((42.58, 42.86), ' relentless')\n",
      "((42.86, 43.24), ' pursuit')\n",
      "((43.24, 43.4), ' of')\n",
      "((43.4, 43.52), ' the')\n",
      "((43.52, 43.76), ' unknown.')\n",
      "((44.48, 44.74), ' And')\n",
      "((44.74, 44.9), ' as')\n",
      "((44.9, 45.0), ' the')\n",
      "((45.0, 45.26), ' story')\n",
      "((45.26, 45.84), ' unfolds,')\n",
      "((46.14, 46.28), ' the')\n",
      "((46.28, 46.5), ' chilling')\n",
      "((46.5, 46.78), ' truth')\n",
      "((46.78, 47.08), ' emerges,')\n",
      "((47.58, 47.68), ' in')\n",
      "((47.68, 47.78), ' the')\n",
      "((47.78, 47.98), ' city')\n",
      "((47.98, 48.12), ' of')\n",
      "((48.12, 48.4), ' shadows,')\n",
      "((48.88, 49.06), ' no')\n",
      "((49.06, 49.22), ' one')\n",
      "((49.22, 49.36), ' is')\n",
      "((49.36, 49.6), ' truly')\n",
      "((49.6, 50.0), ' safe.')\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(speech[0])):\n",
    "    print(((speech[0][l], speech[1][l]), speech[2][l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "\n",
    "def generate_video(): \n",
    "    with open('samp.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    lst = text.split(' ')\n",
    "\n",
    "    clips = []\n",
    "\n",
    "    generator = lambda txt: TextClip(txt, \n",
    "                                fontsize=70, \n",
    "                                color='white', \n",
    "                                bg_color='none', \n",
    "                                font='Arial-Bold',\n",
    "                                method = 'caption',\n",
    "                                size = (1920, 1080))\n",
    "\n",
    "    clips = [CompositeVideoClip([ImageClip('Z_1.jpg').set_duration(0.5).set_position('center')], size=(1920, 1080)) for _ in lst]\n",
    "\n",
    "    subs = [((speech[0][k], speech[1][k]), speech[2][k]) for k in range(len(speech[0]))]\n",
    "        \n",
    "\n",
    "    subtitles = SubtitlesClip(subs, generator)\n",
    "\n",
    "    final = concatenate_videoclips(clips)\n",
    "    audio = AudioFileClip(\"samp.mp3\")\n",
    "    final = final.set_audio(audio)\n",
    "    final_complete = CompositeVideoClip([final, subtitles.set_position(('center', 'bottom'))])\n",
    "    final_complete.write_videofile(\"output0.mp4\", fps=24, codec = 'libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output0.mp4.\n",
      "MoviePy - Writing audio in output0TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output0.mp4\n"
     ]
    }
   ],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tortoise:\n",
      "\n",
      "NAME\n",
      "    tortoise\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    api_fast\n",
      "    do_tts\n",
      "    eval\n",
      "    get_conditioning_latents\n",
      "    is_this_from_tortoise\n",
      "    models (package)\n",
      "    read\n",
      "    read_fast\n",
      "    tts_stream\n",
      "    utils (package)\n",
      "\n",
      "FILE\n",
      "    d:\\projects\\video-generator\\myenv\\lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tortoise as t\n",
    "from tortoise import api_fast\n",
    "\n",
    "help(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mapi_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextToSpeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pcm_audio \u001b[38;5;241m=\u001b[39m tts\u001b[38;5;241m.\u001b[39mtts_with_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour text here\u001b[39m\u001b[38;5;124m\"\u001b[39m, preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfast\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\api_fast.py:216\u001b[0m, in \u001b[0;36mTextToSpeech.__init__\u001b[1;34m(self, autoregressive_batch_size, models_dir, enable_redaction, kv_cache, use_deepspeed, half, device, tokenizer_vocab_file, tokenizer_basic)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive \u001b[38;5;241m=\u001b[39m UnifiedVoice(max_mel_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m604\u001b[39m, max_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m402\u001b[39m, max_conditioning_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    212\u001b[0m                                   model_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    213\u001b[0m                                   heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, number_text_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, start_text_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    214\u001b[0m                                   train_solo_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoregressive\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoregressive.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, models_dir)), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoregressive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init_gpt2_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_deepspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deepspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhifi_decoder \u001b[38;5;241m=\u001b[39m HifiganGenerator(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, resblock_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m resblock_dilation_sizes \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]], resblock_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m11\u001b[39m],\n\u001b[0;32m    220\u001b[0m upsample_kernel_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m], upsample_initial_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, upsample_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m    221\u001b[0m cond_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    222\u001b[0m hifi_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(get_model_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhifidecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32md:\\projects\\Video-Generator\\myenv\\Lib\\site-packages\\tortoise_tts-3.0.0-py3.11.egg\\tortoise\\models\\autoregressive.py:380\u001b[0m, in \u001b[0;36mUnifiedVoice.post_init_gpt2_config\u001b[1;34m(self, use_deepspeed, kv_cache, half)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m GPT2InferenceModel(\n\u001b[0;32m    371\u001b[0m     gpt_config,\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m     kv_cache\u001b[38;5;241m=\u001b[39mkv_cache,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_deepspeed \u001b[38;5;129;01mand\u001b[39;00m half \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine \u001b[38;5;241m=\u001b[39m deepspeed\u001b[38;5;241m.\u001b[39minit_inference(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model,  \n\u001b[0;32m    382\u001b[0m                                             mp_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    383\u001b[0m                                             replace_with_kernel_inject\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    384\u001b[0m                                             dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_engine\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "tts = api_fast.TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)\n",
    "pcm_audio = tts.tts_with_preset(\"your text here\", preset='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
